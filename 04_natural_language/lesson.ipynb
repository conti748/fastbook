{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 4: Practical Deep Learning for Coders - Natural Language (NLP)\n",
    "\n",
    "This notebook presents my personal notes and work coming from my experience with the 4th lesson of the course \"Practical Deep Learning\" by Jeremy Howard ([here](https://www.youtube.com/watch?v=toUgBQv1BT8&list=PLfYUBJiXbdtSvpQjSnJJ_PmDQB_VyT5iU&index=4) for the video on YT) and the related fastbook, the Jupyter-Book from fast.ai. The lecture is partly based on the chapter 10 of the book and the Kaggle notebook available at [Kaggle](https://www.kaggle.com/code/jhoward/getting-started-with-nlp-for-absolute-beginners).\n",
    "\n",
    "The lesson explains how to analyse natural language documents, using Natural Language Processing (NLP). \n",
    "The lecture approaches NLP within the Hugging Face ecosystem, especially the Transformers library, rather than fastai library, and we are going to explain why. \n",
    "\n",
    "Lesson structure:\n",
    "- What is NLP?\n",
    "- Why hugging-face library?\n",
    "- Understanding Fine-tuning;\n",
    "- ULMFiT: the first fine-tuned NLP mode;\n",
    "- Kaggle Competition: classify similarity of phrases used to describe US patents;\n",
    "- Homework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the required packages\n",
    "! pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comment to get warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is NLP?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One area where deep learning has dramatically improved in the last couple of years is natural language processing (NLP). Computers can now generate text, translate automatically from one language to another, analyze comments, label words in sentences, and much more.\n",
    "\n",
    "Perhaps the most widely practically useful application of NLP is classification -- that is, classifying a document automatically into some category. This can be used, for instance, for:\n",
    "\n",
    "- Sentiment analysis (e.g are people saying positive or negative things about your product);\n",
    "- Author identification (what author most likely wrote some document);\n",
    "- Legal discovery (which documents are in scope for a trial);\n",
    "- Organizing documents by topic;\n",
    "- Triaging emails."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why hugging-face library?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lecture zooms in on NLP within the Hugging Face library. Now, you might be thinking, \"Why not stick with good ol' fastai?\" Well, here's the scoop – using more than one library is like having a toolbox with different gadgets. It gives you a broader perspective and helps wrap your head around the concepts from different angles.\n",
    "\n",
    "But why the love for Hugging Face? Simple. It's the big shot in the world of NLP. Let's talk architecture. Unlike fastai, Hugging Face's Transformer doesn't come neatly layered; it's a bit deeper, giving you more control. It's not as plug-and-play as fastai; instead, it's a more hands-on, lower-level library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does it means to fine tune a pre-trained model?\n",
    "We have already done it in the first lecture, but let's understand it.\n",
    "\n",
    "\n",
    "Alright, imagine you've got this pre-trained model, right? It's like a brain that's already seen a bunch of stuff. Now, fine-tuning is like tweaking the settings on that model to make it super smart for a specific task.\n",
    "\n",
    "Let's image us tuning the slide-bar from the previous example.\n",
    "Maybe someone from his experience can share some hints to start our work. The pre-trained model is an external hint – it's decent, but it might not be perfect for your jam. So, you fine-tune it. You slide that bar a bit to make it just right.\n",
    "\n",
    "Now, let's get a bit more techy. Say you trained a model to recognize cats and dogs, but now you want it to tell the difference between penguins and polar bears. You don't wanna start from scratch because that's like reinventing the wheel. Instead, you fine-tune it. You take what it already knows about cats and dogs and give it a crash course on penguins and polar bears.\n",
    "\n",
    "It's like saying, \"Hey brain, you know a lot about animals, but let's focus on these icy fellas for a bit.\" You adjust the model's parameters, those fancy settings that make it tick, to make it a polar bear and penguin expert.\n",
    "\n",
    "As we have seen in the first lecture, the last layers in NN are specific to the task, while the\n",
    "first layers are more generic (edges, corner). In transfer-learning and fine-tuning, we can delete the last layers and substitute it with a random layer and train it on a new specific task!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ULMFiT: the first fine-tuned NLP model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ULMFiT, or Universal Language Model Fine-tuning, as conceptualized by Jeremy and articulated in his [paper](https://arxiv.org/pdf/1801.06146.pdf%C3%AF%C2%BC%E2%80%B0%C3%A3%E2%82%AC%E2%80%9A), stands as a pioneering technique, marking an early instance of fine-tuning in NLP and serving as an influential foundation for subsequent research endeavors.\n",
    "\n",
    "The task in the paper is to analyze sentiment in IMDb reviews, distinguishing between positive and negative sentiments.\n",
    "\n",
    "The problem is divided in 3 steps, as in the figure XXXXXXX:\n",
    "\n",
    "- Step 1: Build a language model (LM) trained on the corpus of Wikipedia. This LM functions as a sophisticated linguistic entity, and is trained to predict subsequent words in Wikipedia articles. Beyond mere lexical comprehension, it endeavors to encapsulate the intricacies of language structure, mathematical formulations, political discourse, logical reasoning, and the nuanced distinction between veracity and fallacy. \n",
    "\n",
    "\n",
    "- Step 2: Enter the fine-tuning phase, involving the training of the language model on IMDb reviews for a limited number of epochs. The model is further refined by predicting subsequent words in IMDb reviews. \n",
    "\n",
    "- Step 3: Subsequently, this refined model is employed to initialize the training of a classifier, constituting the core of the transfer-learning process.\n",
    "\n",
    "Jeremy initially employed Recurrent Neural Networks (RNN), but with the advent of transformers, these models exhibit proficiency in discerning contextual semantics. They are trained to predict words omitted from segments of text.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaggle Competition: classify similarity of phrases used to describe US patents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The competition [\"U.S. Patent Phrase to Phrase Matching\"](https://www.kaggle.com/competitions/us-patent-phrase-to-phrase-matching) asks to build an approach to match phrases in U.S. Patents, here some details:\n",
    "\n",
    "> In this competition, you will train your models on a novel semantic similarity dataset to extract relevant information by matching key phrases in patent documents. Determining the semantic similarity between phrases is critically important during the patent search and examination process to determine if an invention has been described before. For example, if one invention claims \"television set\" and a prior publication describes \"TV set\", a model would ideally recognize these are the same and assist a patent attorney or examiner in retrieving relevant documents. This extends beyond paraphrase identification; if one invention claims a \"strong material\" and another uses \"steel\", that may also be a match. What counts as a \"strong material\" varies per domain (it may be steel in one domain and ripstop fabric in another, but you wouldn't want your parachute made of steel). We have included the Cooperative Patent Classification as the technical domain context as an additional feature to help you disambiguate these situations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's download the data and understand the problem! You need an API key to use the Kaggle API; to get one, click on your profile picture on the Kaggle website, and choose My Account, then click Create New API Token. This will save a file called kaggle.json to your PC. Store it in your HOME directory in the folder kaggle, and accept the competition condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "creds = ''\n",
    "cred_path = Path(r'~/.kaggle/kaggle.json').expanduser()\n",
    "if not cred_path.exists():\n",
    "    cred_path.parent.mkdir(exist_ok=True)\n",
    "    cred_path.write_text(creds)\n",
    "    cred_path.chmod(0o600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading us-patent-phrase-to-phrase-matching.zip to c:\\Users\\conti\\Progetti\\agile-lab\\books\\fastbook\\04_natural_language\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 682k/682k [00:00<00:00, 1.19MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "path = Path('us-patent-phrase-to-phrase-matching')\n",
    "if not path.exists():\n",
    "    import zipfile,kaggle\n",
    "    kaggle.api.competition_download_cli(str(path))\n",
    "    zipfile.ZipFile(f'{path}.zip').extractall(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Documents in NLP datasets are generally in one of two main forms:\n",
    "\n",
    "- Larger documents: One text file per document, often organised into one folder per category\n",
    "- Smaller documents: One document (or document pair, optionally with metadata) per row in a CSV file.\n",
    "\n",
    "Let's look at our data and see what we've got in pandas!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\"ls\" non � riconosciuto come comando interno o esterno,\n",
      " un programma eseguibile o un file batch.\n"
     ]
    }
   ],
   "source": [
    "!ls {path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>anchor</th>\n",
       "      <th>target</th>\n",
       "      <th>context</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>37d61fd2272659b1</td>\n",
       "      <td>abatement</td>\n",
       "      <td>abatement of pollution</td>\n",
       "      <td>A47</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7b9652b17b68b7a4</td>\n",
       "      <td>abatement</td>\n",
       "      <td>act of abating</td>\n",
       "      <td>A47</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>36d72442aefd8232</td>\n",
       "      <td>abatement</td>\n",
       "      <td>active catalyst</td>\n",
       "      <td>A47</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5296b0c19e1ce60e</td>\n",
       "      <td>abatement</td>\n",
       "      <td>eliminating process</td>\n",
       "      <td>A47</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>54c1e3b9184cb5b6</td>\n",
       "      <td>abatement</td>\n",
       "      <td>forest region</td>\n",
       "      <td>A47</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id     anchor                  target context  score\n",
       "0  37d61fd2272659b1  abatement  abatement of pollution     A47   0.50\n",
       "1  7b9652b17b68b7a4  abatement          act of abating     A47   0.75\n",
       "2  36d72442aefd8232  abatement         active catalyst     A47   0.25\n",
       "3  5296b0c19e1ce60e  abatement     eliminating process     A47   0.50\n",
       "4  54c1e3b9184cb5b6  abatement           forest region     A47   0.00"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(path/'train.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>anchor</th>\n",
       "      <th>target</th>\n",
       "      <th>context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>36473</td>\n",
       "      <td>36473</td>\n",
       "      <td>36473</td>\n",
       "      <td>36473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>36473</td>\n",
       "      <td>733</td>\n",
       "      <td>29340</td>\n",
       "      <td>106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>37d61fd2272659b1</td>\n",
       "      <td>component composite coating</td>\n",
       "      <td>composition</td>\n",
       "      <td>H01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>152</td>\n",
       "      <td>24</td>\n",
       "      <td>2186</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id                       anchor       target context\n",
       "count              36473                        36473        36473   36473\n",
       "unique             36473                          733        29340     106\n",
       "top     37d61fd2272659b1  component composite coating  composition     H01\n",
       "freq                   1                          152           24    2186"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe(include='object')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that in the 36473 rows, there are 733 unique anchors, 106 contexts, and nearly 30000 targets. Some anchors are very common, with \"component composite coating\" for instance appearing 152 times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically, in this competition, we are tasked with comparing two words or short phrases, and scoring them based on whether they're similar or not, based on which patent class they were used in. With a score of 1 it is considered that the two inputs have identical meaning, and 0 means they have totally different meaning. For instance, abatement and eliminating process have a score of 0.5, meaning they're somewhat similar, but not identical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to solve this problem? It is not a classification problem and each document is very short (3-4 words), it should be mapped to a simple problem as classification!\n",
    "\n",
    "We could represent the input to the model as something like *\"TEXT1: abatement; TEXT2: eliminating process\"*. We'll need to add the context to this too. In Pandas, we just use + to concatenate, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    TEXT1: A47; TEXT2: abatement of pollution; ANC...\n",
       "1    TEXT1: A47; TEXT2: act of abating; ANC1: abate...\n",
       "2    TEXT1: A47; TEXT2: active catalyst; ANC1: abat...\n",
       "3    TEXT1: A47; TEXT2: eliminating process; ANC1: ...\n",
       "4    TEXT1: A47; TEXT2: forest region; ANC1: abatement\n",
       "Name: input, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['input'] = 'TEXT1: ' + df.context + '; TEXT2: ' + df.target + '; ANC1: ' + df.anchor\n",
    "df.input.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
